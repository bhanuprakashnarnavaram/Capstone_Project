<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lip Movement Detection Recognition</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
            background-color: #e9ecef;
        }

        h1 {
            color: #007bff;
            margin-bottom: 20px;
        }

        #video {
            width: 640px;
            height: 480px;
            border: 2px solid #007bff;
            border-radius: 12px;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
            margin-bottom: 20px;
        }

        #output-text {
            font-size: 20px;
            text-align: center;
            border: 2px solid #007bff;
            padding: 10px;
            width: 80%;
            height: 150px; /* Increased the height */
            border-radius: 8px;
            background-color: #ffffff;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            color: #333;
            margin-top: 10px;
            resize: none; /* Prevents resizing */
        }

        /* Responsive adjustments */
        @media (max-width: 768px) {
            #video {
                width: 100%;
                height: auto;
            }

            #output-text {
                width: 90%;
            }
        }
    </style>
</head>
<body>
    <h1>Lip Movement Detection</h1>
    <video id="video" autoplay></video>
    <textarea id="output-text" rows="5" readonly placeholder="Detected Speech or Lip Movement will appear here..."></textarea>

    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>

    <script>
        const video = document.getElementById('video');
        const outputText = document.getElementById('output-text');

        const lipLandmarksIndexes = [
            61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75, 
            76,  77,  78,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,
            107, 108, 109, 110, 111, 112, 113
        ];

        let lipCoordinates = [];
        let model;
        let recognizing = false;

        // Hidden Speech Recognition
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        recognition.lang = 'en-US';
        recognition.continuous = true;
        recognition.interimResults = false;

        recognition.onstart = () => {
            console.log("Speech recognition started");
        };

        recognition.onresult = (event) => {
            const transcript = event.results[event.results.length - 1][0].transcript.trim();
            outputText.value = transcript;
        };

        recognition.onend = () => {
            if (recognizing) {
                recognition.start();  // Restart recognition if stopped
            }
        };

        async function onResultsFaceMesh(results) {
            if (results.multiFaceLandmarks) {
                for (const landmarks of results.multiFaceLandmarks) {
                    lipCoordinates = lipLandmarksIndexes.map((index) => {
                        return [landmarks[index].x, landmarks[index].y];
                    });

                    // You can call classification function if model is ready
                    const phrase = await classifyLipMovement(lipCoordinates);
                    if (phrase) {
                        outputText.value = phrase;
                    }
                }
            }
        }

        async function loadModel() {
            model = await tf.loadLayersModel('path/to/your/model.json');
        }

        async function classifyLipMovement(lipCoordinates) {
            if (!model) return "";

            const tensorInput = tf.tensor2d(lipCoordinates.flat(), [1, lipCoordinates.length * 2]);
            const predictions = model.predict(tensorInput);

            const predictedIndex = predictions.argMax(-1).dataSync()[0];
            const phrases = ["Hi", "Hello", "Bye", "Thanks", "What are you doing?"];
            return phrases[predictedIndex];
        }

        const faceMesh = new FaceMesh({locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`});
        faceMesh.setOptions({maxNumFaces: 1, refineLandmarks: true, minDetectionConfidence: 0.5, minTrackingConfidence: 0.5});
        faceMesh.onResults(onResultsFaceMesh);

        const camera = new Camera(video, {
            onFrame: async () => {
                await faceMesh.send({image: video});
            },
            width: 640,
            height: 480
        });
        camera.start();

        loadModel();

        // Start speech recognition when the page loads
        recognizing = true;
        recognition.start();
    </script>
</body>
</html>
